# SPDX-License-Identifier: Apache-2.0

# Default values for a production hiero block-node-server deployment
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image:
  repository: ghcr.io/hiero-ledger/hiero-block-node
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
# fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
# runAsUser: 1000

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  requests:
    cpu: "8"
    memory: "24Gi"

nodeSelector: {}

tolerations: []

affinity: {}

blockNode:
  # if blank will use same as AppVersion of chart.
  version: ""
  config:
    # Add any additional env configuration here
    # key: value
    JAVA_TOOL_OPTIONS: "-Djava.util.logging.config.file=/app/logs/config/logging.properties"
    JAVA_OPTS: "-Xms16G -Xmx16G"
    # PRODUCER_TYPE: "NO_OP"
    # PERSISTENCE_STORAGE_TYPE: "NO_OP"
    # VERIFICATION_TYPE: "NO_OP"
    # MEDIATOR_TYPE: "NO_OP"
    MEDIATOR_RING_BUFFER_SIZE: "8192"
  persistence:
    archive:
      # If false, the chart expects an externally provided PVC
      create: true
      # Name of the externally provided PVC
      existingClaim: ""
      # If create is true, the following values are used to create the PVC
      # should match PERSISTENCE_STORAGE_ARCHIVE_ROOT_PATH, leave as is for default.
      mountPath: "/opt/hashgraph/blocknode/data/archive"
      size: 800Gi
      # Optionally add a storage class name if needed
      # storageClass: "your-storage-class"
    live:
      # If false, the chart expects an externally provided PVC
      create: true
      # Name of the externally provided PVC
      existingClaim: ""
      # If create is true, the following values are used to create the PVC
      # should match PERSISTENCE_STORAGE_LIVE_ROOT_PATH, leave as is for default.
      mountPath: "/opt/hashgraph/blocknode/data/live"
      size: 20Gi
      # Optionally add a storage class name if needed
      # storageClass: "your-storage-class"
    unverified:
      # unverified does a hard-coded emptyDir and is ephemeral, however needs to be mounted
      # this does not create a PVC.
      # should match PERSISTENCE_STORAGE_UNVERIFIED_ROOT_PATH, leave as is for default.
      mountPath: "/opt/hashgraph/blocknode/data/unverified"
  secret:
    PRIVATE_KEY: "fake_private_key"
  health:
    readiness:
      endpoint: "/healthz/readyz"
    liveness:
      endpoint: "/healthz/livez"
    metrics:
      port: 9999
  logs:
    # Available Levels are (from most verbose to least verbose):
    # ALL FINEST FINER FINE CONFIG INFO WARNING SEVERE OFF
    level: "INFO"
    loggingProperties:
      # com.hedera.block.server.producer.ProducerBlockItemObserver.level: "FINE"
      io.helidon.webserver.level: "INFO"
      io.helidon.webserver.access.level: "INFO"
      io.helidon.config.level: "SEVERE"
      io.helidon.security.level: "INFO"
      io.helidon.common.level: "INFO"
      handlers: "java.util.logging.ConsoleHandler, java.util.logging.FileHandler"
      java.util.logging.ConsoleHandler.level: "FINE"
      java.util.logging.ConsoleHandler.formatter: "java.util.logging.SimpleFormatter"
      java.util.logging.FileHandler.pattern: "/app/logs/blocknode-%g.log"
      java.util.logging.FileHandler.append: "true"
      java.util.logging.FileHandler.limit: "5_000_000"
      java.util.logging.FileHandler.count: "5"
      java.util.logging.FileHandler.level: "FINE"
      java.util.logging.FileHandler.formatter: "java.util.logging.SimpleFormatter"
      ################################################################################
      # SimpleFormatter single-line format configuration
      ################################################################################
      # The format syntax uses java.util.Formatter.
      # The parameters are:
      #   %1$ - date/time (java.util.Date)
      #   %2$ - source (usually class and method)
      #   %3$ - logger?s name
      #   %4$ - log level
      #   %5$ - log message
      #   %6$ - throwable trace
      java.util.logging.SimpleFormatter.format: "%TF %<TT.%<TL%<Tz %4$-7s [%2$s] %5$s%6$s%n"

kubepromstack:
  enabled: true
  prometheusOperator:
    namespaces:
      releaseNamespace: true
  grafana:
    replicas: 1
    enabled: true
    defaultDashboardsEnabled: false
    adminPassword: "admin"
    datasources:
      datasources.yaml: {}
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        provider:
          allowUiUpdates: true
      datasources:
        enabled: true
        label: grafana_datasource

  nodeExporter:
    enabled: true

loki:
  enabled: true
  isDefault: true
  url: http://{{(include "loki.serviceName" .)}}:{{ .Values.loki.service.port }}
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  datasource:
    jsonData: "{}"
    uid: ""

promtail:
  enabled: true
  config:
    logLevel: info
    serverPort: 3101
    clients:
      - url: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
    snippets:
      pipelineStages:
        - docker:
            label_fields:
              stream: stream
        - multiline:
            # A regex that identifies the start of a new log entry
            firstline: '^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'
            # Maximum wait time for more lines before sending the collected log upstream
            max_wait_time: 3s
            separator: ''
        - replace:
            expression: '(\n){2,}'
            replace: ''

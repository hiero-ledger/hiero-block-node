# SPDX-License-Identifier: Apache-2.0
name: Solo E2E Test

on:
  # Reusable workflow trigger - called by solo-e2e-scheduler.yml
  workflow_call:
    inputs:
      topology:
        description: "Network topology"
        type: string
        default: "single"
      block-node-version:
        description: "Block Node version ('latest', 'rc', 'main', or tag like 'v0.21.2')"
        type: string
        default: "latest"
      consensus-node-version:
        description: "Consensus Node version ('latest', 'rc', or tag like 'v0.68.6')"
        type: string
        default: "latest"
      mirror-node-version:
        description: "Mirror Node version ('latest', 'rc', or tag like 'v0.146.0')"
        type: string
        default: "latest"
      solo-version:
        description: "Solo CLI Version"
        type: string
        default: "0.54.0"
      run-tck-regression-tests:
        description: "Run TCK regression test"
        type: boolean
        default: false
      nlg-enabled:
        description: "Enable NLG load generation"
        type: boolean
        default: false
      nlg-test-type:
        description: "NLG test class: CryptoTransferLoadTest, HCSLoadTest, TokenTransferLoadTest, NftTransferLoadTest"
        type: string
        default: "CryptoTransferLoadTest"
      nlg-args:
        description: "NLG args: -c <clients> -a <accounts> -tt <duration>, e.g., '-c 5 -a 10 -tt 300'"
        type: string
        default: "-c 5 -a 10 -tt 300"
      nlg-max-tps:
        description: "NLG rate limit (optional, uses RateLimitedQueue)"
        type: string
        default: ""
      mirror-node-pinger-tps:
        description: "Mirror Node pinger TPS (0 to disable)"
        type: string
        default: "5"
      test-definition:
        description: "Test definitions (comma-separated): smoke-test, basic-load, node-restart-resilience, full-history-backfill"
        type: string
        default: "none"
    outputs:
      result:
        description: "Test result (success or failure)"
        value: ${{ jobs.solo-test-deploy.outputs.result }}
      resolved-versions:
        description: "Resolved versions (CN,MN,BN)"
        value: ${{ jobs.solo-test-deploy.outputs.resolved-versions }}

  # Manual trigger - for direct invocation from GitHub UI
  workflow_dispatch:
    inputs:
      topology:
        description: "Network topology"
        required: false
        default: "single"
        type: choice
        options:
          - single # 1 CN, 1 BN
          - 2cn-2bn-backfill # 2 CN, 2 BN with backfill
          - paired-3 # 3 CN, 3 BN
          - fan-out-3cn-2bn # 3 CN, 2 BN
          - 3cn-1bn # 3 CN, 1 BN
          - 7cn-3bn-distributed # 7 CN, 3 BN distributed
          - minimal # like single but without MN, Relay and Explorer
      block-node-version:
        description: "Block Node version ('latest', 'rc', 'main', or tag like 'v0.21.2')"
        required: false
        default: "latest"
      consensus-node-version:
        description: "Consensus Node version ('latest', 'rc', 'main', or tag like 'v0.68.6')"
        required: false
        default: "latest"
      mirror-node-version:
        description: "Mirror Node version ('latest', 'rc', 'main', or tag like 'v0.146.0')"
        required: false
        default: "latest"
      solo-version:
        description: "Solo CLI Version"
        required: false
        default: "0.54.0"
      run-tck-regression-tests:
        description: "Run TCK regression test"
        required: false
        default: false
        type: boolean
      nlg-enabled:
        description: "Enable NLG load generation"
        required: false
        default: false
        type: boolean
      nlg-test-type:
        description: "NLG test class"
        required: false
        default: "CryptoTransferLoadTest"
        type: choice
        options:
          - CryptoTransferLoadTest
          - HCSLoadTest
          - TokenTransferLoadTest
          - NftTransferLoadTest
      nlg-args:
        description: "NLG args: -c <clients> -a <accounts> -tt <duration>, e.g., '-c 5 -a 10 -tt 300'"
        required: false
        default: "-c 5 -a 10 -tt 300"
      nlg-max-tps:
        description: "NLG rate limit (optional, uses RateLimitedQueue)"
        required: false
        default: ""
      mirror-node-pinger-tps:
        description: "Mirror Node pinger TPS (0 to disable)"
        required: false
        default: "5"
      test-definition:
        description: "Test definitions (comma-separated): smoke-test, basic-load, node-restart-resilience, full-history-backfill"
        required: false
        default: "none"

defaults:
  run:
    shell: bash

permissions:
  contents: read

concurrency:
  group: solo-network
  cancel-in-progress: true

env:
  GRADLE_EXEC: "ionice -c 2 -n 2 nice -n 19 ./gradlew "
  # Solo ENV
  DEPLOYMENT: "deployment-solo"
  NAMESPACE: "solo-network"
  CLUSTER_NAME: "solo-cluster"
  CONTEXT: "kind-solo-cluster"
  CLUSTER_REFERENCE: "kind-solo-cluster"
  # Solo CLI environment variables
  MIRROR_NODE_PINGER_TPS: ${{ inputs.mirror-node-pinger-tps }}

jobs:
  solo-test-deploy:
    runs-on: hiero-block-node-linux-medium
    outputs:
      result: ${{ steps.result.outputs.status }}
      resolved-versions: ${{ steps.versions.outputs.cn_version }},${{ steps.versions.outputs.mn_version }},${{ steps.versions.outputs.bn_version }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@5ef0c079ce82195b2a36a210272d6b661572d83e # v2.14.2
        with:
          egress-policy: audit

      - name: Checkout repository
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
        with:
          fetch-depth: 0

      # Capture workflow start time for duration summary
      - name: Capture Start Time
        id: timing
        run: |
          echo "workflow_start=$(date +%s)" >> "$GITHUB_OUTPUT"

      - name: Setup Node
        uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v6.2.0
        with:
          node-version: "22"

      - name: Install Helm
        uses: azure/setup-helm@1a275c3b69536ee54be43f2070a358922e12c8d4 # v4.3.1

      - name: Setup Kind
        uses: helm/kind-action@92086f6be054225fa813e0a4b13787fc9088faab # v1.13.0
        with:
          install_only: true
          node_image: kindest/node:v1.31.4@sha256:2cb39f7295fe7eafee0842b1052a599a4fb0f8bcf3f83d96c7f4864c357c6c30
          version: v0.26.0
          kubectl_version: v1.31.4
          verbosity: 3
          wait: 120s

      - name: Install JDK
        uses: actions/setup-java@be666c2fcd27ec809703dec50e508c2fdc7f6654 # v5.2.0
        with:
          distribution: "temurin"
          java-version: "25.0.2"

      # Resolve 'latest' versions to actual GA releases
      - name: Resolve component versions
        id: versions
        run: |
          OUTPUT=$(./tools-and-tests/scripts/solo-e2e-test/scripts/resolve-versions.sh \
            "${{ inputs.consensus-node-version }}" \
            "${{ inputs.mirror-node-version }}" \
            "${{ inputs.block-node-version }}")
          echo "$OUTPUT"
          # Parse output and write to GITHUB_OUTPUT
          echo "$OUTPUT" | grep "^cn_version=" >> "${GITHUB_OUTPUT}"
          echo "$OUTPUT" | grep "^mn_version=" >> "${GITHUB_OUTPUT}"
          echo "$OUTPUT" | grep "^bn_version=" >> "${GITHUB_OUTPUT}"

      # Install Solo CLI
      - name: Install Solo CLI via npm
        id: solo
        run: |
          npm i @hashgraph/solo@${{ inputs.solo-version }} -g
          # Extract version number from solo output (handles various output formats)
          SOLO_INSTALLED=$(solo --version 2>/dev/null | grep -Eo '[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "unknown")
          echo "Solo version: ${SOLO_INSTALLED}"
          echo "solo_version=${SOLO_INSTALLED}" >> "${GITHUB_OUTPUT}"

        # Produce protobuf source artifact
      - name: Produce Protobuf proto artifact
        run: ${GRADLE_EXEC} :protobuf-sources:generateBlockNodeProtoArtifact

        # Untar protobuf sources
      - name: Untar Protobuf Sources
        working-directory: protobuf-sources
        run: |
          mkdir -p proto
          # Use the generated protobuf artifact (matches local project version)
          PROTO_FILE=$(ls block-node-protobuf-*.tgz 2>/dev/null | head -1)
          if [[ -z "$PROTO_FILE" ]]; then
            echo "Error: No protobuf artifact found"
            ls -la
            exit 1
          fi
          echo "Using protobuf artifact: $PROTO_FILE"
          tar -xzf "$PROTO_FILE" -C proto

      - name: Install grpcurl
        run: |
          curl -L https://github.com/fullstorydev/grpcurl/releases/download/v1.8.7/grpcurl_1.8.7_linux_x86_64.tar.gz -o grpcurl.tar.gz
          sudo tar -xzf grpcurl.tar.gz -C /usr/local/bin grpcurl
          rm grpcurl.tar.gz

      - name: Install jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      # Setup cluster using shared script
      - name: Cluster setup (Kind + Solo)
        run: |
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-setup-cluster.sh \
            --cluster-name "${CLUSTER_NAME}" \
            --namespace "${NAMESPACE}" \
            --deployment "${DEPLOYMENT}" \
            --topology "${{ inputs.topology }}" \
            --topologies-dir "${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies"

          TOPOLOGY_FILE="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies/${{ inputs.topology }}.yaml"

          # Add to step summary
          {
            echo "### Cluster Configuration"
            echo ""
            echo "| Setting | Value |"
            echo "|---------|-------|"
            echo "| Topology | ${{ inputs.topology }} |"
            echo "| Cluster Name | ${CLUSTER_NAME} |"
            echo "| Namespace | ${NAMESPACE} |"
            echo ""
            echo "<details>"
            echo "<summary>Topology Definition (${{ inputs.topology }}.yaml)</summary>"
            echo ""
            echo '```yaml'
            cat "$TOPOLOGY_FILE"
            echo '```'
            echo ""
            echo "</details>"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

      # Deploy network using shared script
      - name: Deploy network
        run: |
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-deploy-network.sh \
            --deployment "${DEPLOYMENT}" \
            --namespace "${NAMESPACE}" \
            --cluster-ref "${CLUSTER_REFERENCE}" \
            --topology "${{ inputs.topology }}" \
            --topologies-dir "${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies" \
            --cn-version "${{ steps.versions.outputs.cn_version }}" \
            --mn-version "${{ steps.versions.outputs.mn_version }}" \
            --bn-version "${{ steps.versions.outputs.bn_version }}"

          # Add to step summary - comprehensive version summary
          {
            echo "### Component Versions"
            echo ""
            echo "| Component | Requested | Resolved |"
            echo "|-----------|-----------|----------|"
            echo "| Consensus Node | \`${{ inputs.consensus-node-version || 'latest' }}\` | \`${{ steps.versions.outputs.cn_version }}\` |"
            echo "| Mirror Node | \`${{ inputs.mirror-node-version || 'latest' }}\` | \`${{ steps.versions.outputs.mn_version }}\` |"
            echo "| Block Node | \`${{ inputs.block-node-version || 'latest' }}\` | \`${{ steps.versions.outputs.bn_version }}\` |"
            echo "| Solo CLI | \`${{ inputs.solo-version || 'latest' }}\` | \`${{ steps.solo.outputs.solo_version }}\` |"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

      - name: Show running pods and services
        run: |
          kubectl --context "${CONTEXT}" get pods -n ${NAMESPACE}
          kubectl --context "${CONTEXT}" get svc -n ${NAMESPACE}

      # Port forwards - dynamically discover and forward all nodes
      - name: Solo Port forwards
        id: deploy_end
        run: |
          ${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/scripts/solo-port-forward.sh \
            --namespace "${NAMESPACE}"
          echo "deploy_end=$(date +%s)" >> "$GITHUB_OUTPUT"

      # ---- Test Definition Runner (optional) ----
      # Run a test definition file if specified. This provides a structured
      # way to define timed events and assertions for E2E testing.

      # Validate all test definitions against topology before running (fail fast)
      - name: Validate Test Definitions
        if: ${{ inputs.test-definition != 'none' && inputs.test-definition != '' }}
        run: |
          TESTS_DIR="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/tests"
          TOPOLOGIES_DIR="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies"
          TEST_RUNNER="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/scripts/solo-test-runner.sh"

          IFS=',' read -ra TESTS <<< "${{ inputs.test-definition }}"
          echo "Validating ${#TESTS[@]} test(s) against topology '${{ inputs.topology }}'..."

          for test in "${TESTS[@]}"; do
            test=$(echo "$test" | xargs)  # trim whitespace
            [[ -z "$test" || "$test" == "none" ]] && continue
            echo "Validating: $test"
            if ! "${TEST_RUNNER}" --test "${TESTS_DIR}/${test}.yaml" --topology "${{ inputs.topology }}" --topologies-dir "${TOPOLOGIES_DIR}" --validate; then
              echo "::error::Test '$test' is not compatible with topology '${{ inputs.topology }}'"
              exit 1
            fi
          done
          echo "All tests validated successfully"

      # ---- Load Generation with NLG (Optional) ----
      # NLG is synchronous - runs for the specified duration then completes

      - name: Run Rapid-Fire (NLG) Load Test
        if: ${{ inputs.nlg-enabled }}
        env:
          NLG_TEST_TYPE: ${{ inputs.nlg-test-type }}
          NLG_ARGS: ${{ inputs.nlg-args }}
          NLG_MAX_TPS: ${{ inputs.nlg-max-tps }}
        run: |
          NLG_START_TIME=$(date +%s)
          NLG_OUTPUT_FILE=$(mktemp)

          # Run load test (synchronous - runs for duration specified in args)
          # Capture output for summary while also displaying it
          NLG_EXIT_CODE=0
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-load-generate.sh start 2>&1 | tee "$NLG_OUTPUT_FILE" || NLG_EXIT_CODE=$?

          # Cleanup
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-load-generate.sh stop 2>&1 | tee -a "$NLG_OUTPUT_FILE" || true

          NLG_END_TIME=$(date +%s)
          NLG_DURATION=$((NLG_END_TIME - NLG_START_TIME))

          # Format duration
          NLG_MINS=$((NLG_DURATION / 60))
          NLG_SECS=$((NLG_DURATION % 60))

          # Determine result
          if [[ $NLG_EXIT_CODE -eq 0 ]]; then
            NLG_RESULT=":white_check_mark: Completed"
          else
            NLG_RESULT=":x: Failed (exit code: $NLG_EXIT_CODE)"
          fi

          # Add to step summary
          {
            echo "### NLG Load Test Results"
            echo ""
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Result** | $NLG_RESULT |"
            echo "| Test Type | \`${{ inputs.nlg-test-type }}\` |"
            echo "| NLG Args | \`${{ inputs.nlg-args }}\` |"
            echo "| Max TPS | ${{ inputs.nlg-max-tps || 'unlimited' }} |"
            echo "| Duration | ${NLG_MINS}m ${NLG_SECS}s |"
            echo ""
            echo "<details>"
            echo "<summary>NLG Output</summary>"
            echo ""
            echo '```'
            cat "$NLG_OUTPUT_FILE"
            echo '```'
            echo ""
            echo "</details>"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          rm -f "$NLG_OUTPUT_FILE"

          # Fail the step if NLG failed
          exit $NLG_EXIT_CODE

      # Run each test definition sequentially on the same deployment
      - name: Run Test Definitions
        id: tests_end
        if: ${{ inputs.test-definition != 'none' && inputs.test-definition != '' }}
        run: |
          TESTS_DIR="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/tests"
          TOPOLOGIES_DIR="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies"
          TEST_RUNNER="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/scripts/solo-test-runner.sh"
          RESULTS_FILE="${{ github.workspace }}/test-results.txt"

          IFS=',' read -ra TESTS <<< "${{ inputs.test-definition }}"
          FAILED=0
          PASSED=0

          # Initialize results file
          echo "" > "$RESULTS_FILE"

          for test in "${TESTS[@]}"; do
            test=$(echo "$test" | xargs)  # trim whitespace
            [[ -z "$test" || "$test" == "none" ]] && continue

            echo ""
            echo "=========================================="
            echo "Running test: $test"
            echo "=========================================="

            if "${TEST_RUNNER}" \
              --test "${TESTS_DIR}/${test}.yaml" \
              --topology "${{ inputs.topology }}" \
              --namespace "${NAMESPACE}" \
              --context "${CONTEXT}" \
              --topologies-dir "${TOPOLOGIES_DIR}" \
              --proto-path "protobuf-sources/proto" \
              --output github-summary; then
              echo "${test}=passed" >> "$RESULTS_FILE"
              PASSED=$((PASSED + 1))
            else
              echo "::warning::Test '$test' failed"
              echo "${test}=failed" >> "$RESULTS_FILE"
              FAILED=$((FAILED + 1))
            fi
          done

          # Export counts for summary
          echo "TEST_RESULTS_FILE=$RESULTS_FILE" >> "$GITHUB_ENV"
          echo "TESTS_PASSED=$PASSED" >> "$GITHUB_ENV"
          echo "TESTS_FAILED=$FAILED" >> "$GITHUB_ENV"
          echo "TESTS_TOTAL=$((PASSED + FAILED))" >> "$GITHUB_ENV"

          if [[ $FAILED -gt 0 ]]; then
            echo "::error::$FAILED test(s) failed"
            exit 1
          fi

          # Brief cooldown to let streaming connections settle before grpcurl validation
          echo "Waiting 15s for streaming connections to settle..."
          sleep 15

          # Capture tests end time
          echo "tests_end=$(date +%s)" >> "$GITHUB_OUTPUT"

      # ---- BN verification with grpcurl ----
      # Skip grpcurl validation when test definitions ran (they include assertions)
      - name: Get ServerStatus from Block Node
        if: ${{ inputs.test-definition == 'none' || inputs.test-definition == '' }}
        run: |
          # Call serverStatus once
          STATUS_JSON=$(grpcurl \
            -plaintext \
            -emit-defaults \
            -import-path protobuf-sources/proto \
            -proto block-node/api/node_service.proto \
            -d '{}' \
            localhost:40840 \
            org.hiero.block.api.BlockNodeService/serverStatus)

          # Extract values
          FIRST_AVAILABLE_BLOCK=$(echo "$STATUS_JSON" | jq -r '.firstAvailableBlock')
          LAST_AVAILABLE_BLOCK=$(echo "$STATUS_JSON" | jq -r '.lastAvailableBlock')

          echo "First available block is $FIRST_AVAILABLE_BLOCK"
          echo "Last available block is $LAST_AVAILABLE_BLOCK"

          # Validate
          if [[ "$FIRST_AVAILABLE_BLOCK" != "0" ]]; then
              echo "First available block is not 0"
              exit 1
          fi
          if (( LAST_AVAILABLE_BLOCK < 1 )); then
              echo "Last available block is less than 1"
              exit 1
          fi

          # Export for later steps
          echo "FIRST_AVAILABLE_BLOCK=$FIRST_AVAILABLE_BLOCK" >> $GITHUB_ENV
          echo "LAST_AVAILABLE_BLOCK=$LAST_AVAILABLE_BLOCK" >> $GITHUB_ENV

      - name: Get first block from Block Node
        if: ${{ inputs.test-definition == 'none' || inputs.test-definition == '' }}
        continue-on-error: true # Proto version mismatch (BN 0.69 vs CN 0.70) causes parsing failures
        run: |
          # Retry logic for transient gRPC errors (e.g., flow control under load)
          MAX_RETRIES=3
          RETRY_DELAY=10
          for attempt in $(seq 1 $MAX_RETRIES); do
            echo "Attempt $attempt of $MAX_RETRIES: Getting block $FIRST_AVAILABLE_BLOCK"
            FIRST_BLOCK=$(grpcurl \
              -plaintext \
              -emit-defaults \
              -max-msg-sz 268435456 \
              -import-path protobuf-sources/proto \
              -proto block-node/api/block_access_service.proto \
              -d "{\"block_number\": $FIRST_AVAILABLE_BLOCK}" \
              localhost:40840 \
              org.hiero.block.api.BlockAccessService/getBlock 2>&1 \
              | jq -r '.block.items[0].blockHeader.number' 2>/dev/null) || true

            if [[ -n "$FIRST_BLOCK" && "$FIRST_BLOCK" != "null" ]]; then
              echo "FirstBlock is ${FIRST_BLOCK}"
              exit 0
            fi

            if [[ $attempt -lt $MAX_RETRIES ]]; then
              echo "Retrying in ${RETRY_DELAY}s..."
              sleep $RETRY_DELAY
            fi
          done

          echo "Error: Failed to get first block after $MAX_RETRIES attempts"
          exit 1

      - name: Get last block from Block Node
        if: ${{ inputs.test-definition == 'none' || inputs.test-definition == '' }}
        continue-on-error: true # Proto version mismatch (BN 0.69 vs CN 0.70) causes parsing failures
        run: |
          # Retry logic for transient gRPC errors (e.g., flow control under load)
          MAX_RETRIES=3
          RETRY_DELAY=10
          for attempt in $(seq 1 $MAX_RETRIES); do
            echo "Attempt $attempt of $MAX_RETRIES: Getting block $LAST_AVAILABLE_BLOCK"
            LAST_BLOCK=$(grpcurl \
              -plaintext \
              -emit-defaults \
              -max-msg-sz 268435456 \
              -import-path protobuf-sources/proto \
              -proto block-node/api/block_access_service.proto \
              -d "{\"block_number\": $LAST_AVAILABLE_BLOCK}" \
              localhost:40840 \
              org.hiero.block.api.BlockAccessService/getBlock 2>&1 \
              | jq -r '.block.items[0].blockHeader.number' 2>/dev/null) || true

            if [[ -n "$LAST_BLOCK" && "$LAST_BLOCK" != "null" ]]; then
              echo "LastBlock is ${LAST_BLOCK}"
              exit 0
            fi

            if [[ $attempt -lt $MAX_RETRIES ]]; then
              echo "Retrying in ${RETRY_DELAY}s..."
              sleep $RETRY_DELAY
            fi
          done

          echo "Error: Failed to get last block after $MAX_RETRIES attempts"
          exit 1

      # Skip MN validation when tests ran or topology has no Mirror Node (minimal)
      - name: Validate first block on Mirror Node
        if: ${{ (inputs.test-definition == 'none' || inputs.test-definition == '') && !contains(inputs.topology, 'minimal') }}
        run: |
          # Fetch the block number from Mirror Node
          FIRST_BLOCK_MIRROR=$(curl -s "http://127.0.0.1:5551/api/v1/blocks/$FIRST_AVAILABLE_BLOCK" | jq -r '.number')

          # Check if we got a value
          if [ -z "$FIRST_BLOCK_MIRROR" ]; then
            echo "Error: FIRST_BLOCK_MIRROR is empty"
            exit 1
          fi

          echo "FIRST_BLOCK_MIRROR is $FIRST_BLOCK_MIRROR"
          echo "FIRST_AVAILABLE_BLOCK is $FIRST_AVAILABLE_BLOCK"

          # Validate that the Mirror Node block matches the Block Node first block
          if [ "$FIRST_BLOCK_MIRROR" != "$FIRST_AVAILABLE_BLOCK" ]; then
            echo "Error: Mirror Node first block ($FIRST_BLOCK_MIRROR) does not match Block Node first block ($FIRST_AVAILABLE_BLOCK)"
            exit 1
          fi

          echo "Mirror Node first block matches Block Node first block"

      - name: Validate last block on Mirror Node
        if: ${{ (inputs.test-definition == 'none' || inputs.test-definition == '') && !contains(inputs.topology, 'minimal') }}
        run: |
          # Fetch the block number from Mirror Node
          LAST_BLOCK_MIRROR=$(curl -s "http://127.0.0.1:5551/api/v1/blocks/$LAST_AVAILABLE_BLOCK" | jq -r '.number')

          # Check if we got a value
          if [ -z "$LAST_BLOCK_MIRROR" ]; then
              echo "Error: LAST_BLOCK_MIRROR is empty"
              exit 1
          fi

          echo "LAST_BLOCK_MIRROR is $LAST_BLOCK_MIRROR"
          echo "LAST_AVAILABLE_BLOCK is $LAST_AVAILABLE_BLOCK"

          # Validate that the Mirror Node block matches the Block Node last block
          if [ "$LAST_BLOCK_MIRROR" != "$LAST_AVAILABLE_BLOCK" ]; then
              echo "Error: Mirror Node last block ($LAST_BLOCK_MIRROR) does not match Block Node last block ($LAST_AVAILABLE_BLOCK)"
              exit 1
          fi

          echo "Mirror Node last block matches Block Node last block"

      # Test Duration Summary - shows timing breakdown
      - name: Test Duration Summary
        if: always()
        run: |
          END_TIME=$(date +%s)
          START_TIME="${{ steps.timing.outputs.workflow_start }}"
          DEPLOY_END="${{ steps.deploy_end.outputs.deploy_end }}"
          TESTS_END="${{ steps.tests_end.outputs.tests_end }}"

          # Calculate durations (with fallbacks for missing timestamps)
          SETUP_DURATION=0
          DEPLOY_DURATION=0
          TEST_DURATION=0

          if [[ -n "$START_TIME" && -n "$DEPLOY_END" ]]; then
            SETUP_DURATION=$((DEPLOY_END - START_TIME))
          fi

          if [[ -n "$TESTS_END" && -n "$DEPLOY_END" ]]; then
            TEST_DURATION=$((TESTS_END - DEPLOY_END))
          fi

          if [[ -n "$START_TIME" ]]; then
            TOTAL_DURATION=$((END_TIME - START_TIME))
          else
            TOTAL_DURATION=0
          fi

          # Format as minutes:seconds
          format_duration() {
            local seconds=$1
            local minutes=$((seconds / 60))
            local secs=$((seconds % 60))
            printf "%dm %02ds" "$minutes" "$secs"
          }

          {
            echo "### Test Duration"
            echo ""
            echo "| Phase | Duration |"
            echo "|-------|----------|"
            echo "| Setup & Deploy | $(format_duration $SETUP_DURATION) |"
            if [[ -n "$TESTS_END" ]]; then
              echo "| Tests | $(format_duration $TEST_DURATION) |"
            fi
            echo "| **Total** | **$(format_duration $TOTAL_DURATION)** |"
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"

      # TCK Regression Tests related steps (matches consensus-node approach)
      - name: Setup PNPM
        if: ${{ inputs.run-tck-regression-tests }}
        run: |
          npm install -g pnpm

      # Checkout TCK and checkout latest tag
      - name: Checkout TCK SDK
        if: ${{ inputs.run-tck-regression-tests }}
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
        with:
          path: sdk-tck/regression
          repository: hiero-ledger/hiero-sdk-tck
          fetch-depth: 0
          fetch-tags: true

      - name: Checkout Latest Tag (TCK)
        if: ${{ inputs.run-tck-regression-tests }}
        run: |
          cd sdk-tck/regression
          git fetch --tags
          LATEST_TAG=$(git describe --tags --abbrev=0)
          echo "Checking out TCK latest tag: $LATEST_TAG"
          git checkout "$LATEST_TAG"

      # Checkout JS-SDK and checkout latest tag
      - name: Checkout JS-SDK Server
        if: ${{ inputs.run-tck-regression-tests }}
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
        with:
          path: sdk-tck/sdk-server
          repository: hiero-ledger/hiero-sdk-js
          fetch-depth: 0
          fetch-tags: true

      - name: Checkout Latest Tag (JS-SDK)
        if: ${{ inputs.run-tck-regression-tests }}
        run: |
          cd sdk-tck/sdk-server
          git fetch --tags
          LATEST_TAG=$(git describe --tags --abbrev=0)
          echo "Checking out JS-SDK latest tag: $LATEST_TAG"
          git checkout "$LATEST_TAG"

      # Install TCK-SDK dependencies using shared script (versions extracted from source)
      - name: Install TCK-SDK Dependencies
        if: ${{ inputs.run-tck-regression-tests }}
        run: |
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-tck-sdk-install.sh \
            --sdk-dir "sdk-tck/sdk-server" \
            --tck-dir "sdk-tck/regression"

      # Run TCK-SDK tests using shared script
      - name: Run TCK-SDK Tests
        if: ${{ inputs.run-tck-regression-tests }}
        continue-on-error: true # Allow workflow to continue; results shown in TCK-SDK Results Summary
        run: |
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-tck-sdk-run.sh \
            --sdk-dir "sdk-tck/sdk-server" \
            --tck-dir "sdk-tck/regression" \
            --deployment "${DEPLOYMENT}" \
            --namespace "${NAMESPACE}"

      # Add TCK-SDK Results to Summary
      - name: TCK-SDK Results Summary
        if: ${{ !cancelled() && inputs.run-tck-regression-tests }}
        run: |
          REPORT_FILE="sdk-tck/regression/mochawesome-report/mochawesome.json"

          if [[ ! -f "$REPORT_FILE" ]]; then
            echo "::warning::TCK-SDK report not found at $REPORT_FILE"
            {
              echo "### TCK-SDK Test Results"
              echo ""
              echo ":warning: Report not found - tests may not have run"
              echo ""
            } >> "${GITHUB_STEP_SUMMARY}"
            exit 0
          fi

          # Parse mochawesome.json for test results
          TOTAL=$(jq -r '.stats.tests // 0' "$REPORT_FILE")
          PASSED=$(jq -r '.stats.passes // 0' "$REPORT_FILE")
          FAILED=$(jq -r '.stats.failures // 0' "$REPORT_FILE")
          PENDING=$(jq -r '.stats.pending // 0' "$REPORT_FILE")
          DURATION_MS=$(jq -r '.stats.duration // 0' "$REPORT_FILE")
          DURATION_S=$((DURATION_MS / 1000))

          # Determine overall result
          if [[ $FAILED -eq 0 && $TOTAL -gt 0 ]]; then
            TCK_RESULT=":white_check_mark: $PASSED passed"
          elif [[ $FAILED -gt 0 ]]; then
            TCK_RESULT=":x: $PASSED passed, $FAILED failed"
          else
            TCK_RESULT=":warning: No tests executed"
          fi

          # Add summary section
          {
            echo "### TCK-SDK Test Results"
            echo ""
            echo "| Metric | Value |"
            echo "|--------|-------|"
            echo "| **Result** | $TCK_RESULT |"
            echo "| Total Tests | $TOTAL |"
            echo "| Passed | $PASSED |"
            echo "| Failed | $FAILED |"
            echo "| Pending/Skipped | $PENDING |"
            echo "| Duration | ${DURATION_S}s |"
            echo ""
          } >> "${GITHUB_STEP_SUMMARY}"

          # List failed tests if any
          if [[ $FAILED -gt 0 ]]; then
            {
              echo "<details>"
              echo "<summary>Failed Tests</summary>"
              echo ""
              echo '```'
              # Extract failed test titles from the report
              jq -r '.results[].suites[]?.tests[]? | select(.fail == true) | "- \(.fullTitle)"' "$REPORT_FILE" 2>/dev/null || echo "Unable to parse failed tests"
              echo '```'
              echo ""
              echo "</details>"
              echo ""
            } >> "${GITHUB_STEP_SUMMARY}"
          fi

      - name: BN Based - SDK TCK Regression Test Report
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        if: ${{ !cancelled()  &&  inputs.run-tck-regression-tests }}
        with:
          name: SDK TCK Regression Test Report
          path: "**/sdk-tck/regression/mochawesome-report/mochawesome.*"
          retention-days: 7

      # Network Health Summary - show status of all nodes (after all tests)
      - name: Network Health Summary
        if: always()
        continue-on-error: true
        run: |
          ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-network-status.sh \
            --namespace "${NAMESPACE}" \
            --topology "${{ inputs.topology }}" \
            --topologies-dir "${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies" \
            --context "${CONTEXT}" \
            --proto-path "protobuf-sources/proto" \
            --output github-summary

      # Block Node Metrics Summary - show metrics for each block node (after all tests)
      - name: Block Node Metrics Summary
        if: always()
        continue-on-error: true
        run: |
          TOPOLOGY_FILE="${{ github.workspace }}/tools-and-tests/scripts/solo-e2e-test/topologies/${{ inputs.topology }}.yaml"
          BLOCK_NODES=$(grep -E '^[[:space:]]+block-node-[0-9]+:' "${TOPOLOGY_FILE}" | sed 's/://g' | awk '{print $1}' || echo "block-node-1")

          PORT=16007
          for bn in ${BLOCK_NODES}; do
            echo "<details>" >> "${GITHUB_STEP_SUMMARY}"
            echo "<summary>ðŸ“Š ${bn} Metrics</summary>" >> "${GITHUB_STEP_SUMMARY}"
            echo "" >> "${GITHUB_STEP_SUMMARY}"
            ./tools-and-tests/scripts/solo-e2e-test/scripts/solo-metrics-summary.sh \
              ${PORT} github-summary >> "${GITHUB_STEP_SUMMARY}" || echo "Metrics unavailable for ${bn}" >> "${GITHUB_STEP_SUMMARY}"
            echo "</details>" >> "${GITHUB_STEP_SUMMARY}"
            echo "" >> "${GITHUB_STEP_SUMMARY}"
            PORT=$((PORT + 1))
          done

      - name: Collect BN and MN Logs
        if: always()
        continue-on-error: true
        run: |
          # Create log directories
          mkdir -p bn-logs mn-logs

          # Get BN Logs for ALL block nodes dynamically
          for svc in $(kubectl --context "${CONTEXT}" get svc -n "${NAMESPACE}" -o name 2>/dev/null | grep "block-node-" | sort); do
            bn_name=$(basename "$svc")

            # Get stdout/stderr logs via kubectl logs
            kubectl --context "${CONTEXT}" logs -n "${NAMESPACE}" -l "app.kubernetes.io/name=${bn_name}" \
              --all-containers --since=24h --timestamps --prefix --tail=-1 > "bn-logs/${bn_name}-stdout.log" 2>&1 || \
              echo "Failed to collect stdout logs for ${bn_name}" > "bn-logs/${bn_name}-stdout.log"

            # Get file logs from inside the container using kubectl cp
            # Block Node writes logs to /opt/hiero/block-node/logs/
            POD_NAME=$(kubectl --context "${CONTEXT}" get pods -n "${NAMESPACE}" -l "app.kubernetes.io/name=${bn_name}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [[ -n "$POD_NAME" ]]; then
              mkdir -p "bn-logs/${bn_name}-files"
              kubectl --context "${CONTEXT}" cp "${NAMESPACE}/${POD_NAME}:/opt/hiero/block-node/logs/" "bn-logs/${bn_name}-files/" 2>/dev/null || \
                echo "No file logs available for ${bn_name}" > "bn-logs/${bn_name}-files/no-logs.txt"
              echo "Collected file logs for ${bn_name} from pod ${POD_NAME}"
            fi

            echo "Collected logs for ${bn_name}"
          done

          # Get MN Logs - stdout/stderr
          kubectl --context "${CONTEXT}" logs -n "${NAMESPACE}" -l "app.kubernetes.io/instance=mirror-1,app.kubernetes.io/component=importer" \
            --all-containers --since=24h --timestamps --prefix --tail=-1 > "mn-logs/mirror-1-importer-stdout.log" 2>&1 || \
            echo "Failed to collect MN importer stdout logs" > "mn-logs/mirror-1-importer-stdout.log"

          # Get MN file logs from importer pod
          MN_POD=$(kubectl --context "${CONTEXT}" get pods -n "${NAMESPACE}" -l "app.kubernetes.io/instance=mirror-1,app.kubernetes.io/component=importer" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [[ -n "$MN_POD" ]]; then
            mkdir -p "mn-logs/importer-files"
            # Mirror Node importer typically logs to /var/log or container logs
            kubectl --context "${CONTEXT}" cp "${NAMESPACE}/${MN_POD}:/var/log/" "mn-logs/importer-files/" 2>/dev/null || true
            echo "Collected file logs for mirror-1-importer from pod ${MN_POD}"
          fi

          # Show what was collected
          echo "=== Collected BN logs ===" && ls -laR bn-logs/
          echo "=== Collected MN logs ===" && ls -laR mn-logs/

      - name: Collect CN Logs (BN-focused)
        if: always()
        run: |
          # Get CN Logs (Solo 0.52+ uses 'deployment diagnostics' instead of 'consensus diagnostics')
          solo deployment diagnostics all --dev --deployment "${DEPLOYMENT}" || \
            echo "Failed to collect CN diagnostics (deployment may be unavailable)"
          LOG_FOLDER="${HOME}/.solo/logs"

          # Create filtered CN logs directory - extract only BN-relevant files
          # This reduces artifact size from GB to MB by excluding:
          # - solo.log/solo.ndjson (1.5+ TB of Solo CLI operations)
          # - Most CN zip contents (consensus internals, state logs)
          mkdir -p cn-logs-filtered

          # Extract only BN-relevant files from each CN diagnostic zip
          for zip in "${LOG_FOLDER}/${NAMESPACE}"/*.zip; do
            [[ -f "$zip" ]] || continue
            node_name=$(basename "$zip" .zip)
            mkdir -p "cn-logs-filtered/${node_name}"

            # blocknode-comms.log - Block stream communication to BNs
            unzip -j -o "$zip" "*/blocknode-comms.log" -d "cn-logs-filtered/${node_name}/" 2>/dev/null || true
            # settingsUsed.txt - CN config including BN routing
            unzip -j -o "$zip" "*/settingsUsed.txt" -d "cn-logs-filtered/${node_name}/" 2>/dev/null || true
            # grpc-access.log - gRPC access logs (may include BN connections)
            unzip -j -o "$zip" "*/grpc-access.log" -d "cn-logs-filtered/${node_name}/" 2>/dev/null || true
          done

          # Capture last 1000 lines of solo.log (recent activity only, not full history)
          tail -1000 "${LOG_FOLDER}/solo.log" > "cn-logs-filtered/solo-tail.log" 2>/dev/null || true

          echo "CN_LOG_FOLDER=cn-logs-filtered" >> $GITHUB_ENV

          # Show what was collected
          echo "=== Collected CN logs (BN-relevant only) ===" && ls -laR cn-logs-filtered/

      - name: Upload CN Logs
        if: always()
        id: upload_logs_cn
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: cn-logs
          path: ${{ env.CN_LOG_FOLDER }}/

      - name: Upload BN Logs
        if: always()
        continue-on-error: true
        id: upload_logs_bn
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: BN-logs
          path: bn-logs/

      - name: Upload MN Logs
        if: always()
        continue-on-error: true
        id: upload_logs_mn
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: MN-logs
          path: mn-logs/

      - name: Add Logs and Status to Summary
        if: always()
        continue-on-error: true
        run: |
          {
            echo "### Logs"
            echo ""
            echo "| Log | Download |"
            echo "|-----|----------|"
            echo "| Block Node | [BN-logs](${{ steps.upload_logs_bn.outputs.artifact-url }}) |"
            echo "| Mirror Node | [MN-logs](${{ steps.upload_logs_mn.outputs.artifact-url }}) |"
            echo "| Consensus Node | [CN-logs](${{ steps.upload_logs_cn.outputs.artifact-url }}) |"
            echo ""

            # Test Results Matrix (if tests were run)
            if [[ -f "$TEST_RESULTS_FILE" && -s "$TEST_RESULTS_FILE" ]]; then
              echo "---"
              echo ""
              echo "### Test Results"
              echo ""
              echo "| Test | Result |"
              echo "|------|--------|"
              while IFS='=' read -r test_name result; do
                [[ -z "$test_name" ]] && continue
                if [[ "$result" == "passed" ]]; then
                  echo "| \`${test_name}\` | :white_check_mark: Passed |"
                else
                  echo "| \`${test_name}\` | :x: Failed |"
                fi
              done < "$TEST_RESULTS_FILE"
              echo ""
              echo "**Summary:** ${TESTS_PASSED:-0} passed, ${TESTS_FAILED:-0} failed (${TESTS_TOTAL:-0} total)"
              echo ""
            fi

            echo "---"
            echo ""
            echo "### Overall Result"
            echo ""
            if [[ "${{ job.status }}" == "success" ]]; then
              echo "## :white_check_mark: PASSED"
            else
              echo "## :x: FAILED"
            fi
            echo ""
          } >> "$GITHUB_STEP_SUMMARY"

      # Set result output for workflow_call consumers
      - name: Set result output
        id: result
        if: always()
        run: |
          if [[ "${{ job.status }}" == "success" ]]; then
            echo "status=success" >> "$GITHUB_OUTPUT"
          else
            echo "status=failure" >> "$GITHUB_OUTPUT"
          fi

      # Stop the solo nodes
      - name: Stop solo
        if: ${{ always() }}
        run: |
          kind delete cluster -n ${{ env.CLUSTER_NAME }}

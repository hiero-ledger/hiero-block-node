# SPDX-License-Identifier: Apache-2.0
# Solo E2E Test Taskfile - Local development helper for Hiero network deployments
#
# Prerequisites:
#   - Docker (for Kind)
#   - kubectl
#   - helm
#   - kind
#   - solo CLI (npm i @hashgraph/solo -g)
#   - grpcurl (optional, for verification)
#
# Quick start:
#   1. Copy .env.example to .env and set TOPOLOGY (optional)
#   2. Run: task up
#   3. When done: task down
#
# Configuration (via .env or environment):
#   TOPOLOGY      - Network topology (default: single)
#   CLUSTER_NAME  - Kind cluster name (default: solo-cluster)
#   CN_VERSION    - Consensus Node version (default: latest)
#   MN_VERSION    - Mirror Node version (default: latest)
#   BN_VERSION    - Block Node version (default: latest)
#   NLG_CONCURRENCY - NLG -c parameter (default: 5)
#   NLG_ACCOUNTS    - NLG -a parameter (default: 10)
#   NLG_DURATION    - NLG -t parameter in seconds (default: 300)

version: '3'

dotenv: ['.env']

vars:
  # Paths (relative to this Taskfile)
  SCRIPTS_DIR: "{{.ROOT_DIR}}/scripts"
  TOPOLOGIES_DIR: "{{.ROOT_DIR}}/topologies"
  TOPOLOGY_TOOL: "{{.ROOT_DIR}}/../network-topology-tool"
  PROTO_PATH: "{{.ROOT_DIR}}/../../../protobuf-sources/src/main/proto"

  # Config with defaults (override via .env or environment)
  TOPOLOGY: '{{.TOPOLOGY | default "single"}}'
  CLUSTER_NAME: '{{.CLUSTER_NAME | default "solo-cluster"}}'
  NAMESPACE: '{{.NAMESPACE | default "solo-network"}}'
  DEPLOYMENT: '{{.DEPLOYMENT | default "deployment-solo"}}'
  CLUSTER_REF: 'kind-{{.CLUSTER_NAME}}'

  # Version defaults
  CN_VERSION: '{{.CN_VERSION | default "latest"}}'
  MN_VERSION: '{{.MN_VERSION | default "latest"}}'
  BN_VERSION: '{{.BN_VERSION | default "latest"}}'

  # NLG load generation parameters (direct, no TPS abstraction)
  NLG_TEST_CLASS: '{{.NLG_TEST_CLASS | default "CryptoTransferLoadTest"}}'
  NLG_CONCURRENCY: '{{.NLG_CONCURRENCY | default "5"}}'
  NLG_ACCOUNTS: '{{.NLG_ACCOUNTS | default "10"}}'
  NLG_DURATION: '{{.NLG_DURATION | default "300"}}'
  NLG_EXTRA_ARGS: '{{.NLG_EXTRA_ARGS | default ""}}'

tasks:
  # ============================================================================
  # Prerequisites
  # ============================================================================
  check:
    desc: Check all prerequisites are installed
    cmds:
      - |
        echo "Checking prerequisites..."
        command -v docker >/dev/null 2>&1 || { echo "ERROR: docker not found"; exit 1; }
        command -v kubectl >/dev/null 2>&1 || { echo "ERROR: kubectl not found"; exit 1; }
        command -v helm >/dev/null 2>&1 || { echo "ERROR: helm not found"; exit 1; }
        command -v kind >/dev/null 2>&1 || { echo "ERROR: kind not found"; exit 1; }
        command -v solo >/dev/null 2>&1 || { echo "ERROR: solo not found. Install with: npm i @hashgraph/solo -g"; exit 1; }
        echo "All prerequisites found!"
        echo ""
        echo "Versions:"
        echo "  Docker: $(docker --version)"
        echo "  kubectl: $(kubectl version --client --short 2>/dev/null || kubectl version --client)"
        echo "  Helm: $(helm version --short)"
        echo "  Kind: $(kind --version)"
        echo "  Solo: $(solo --version)"
    silent: true

  # ============================================================================
  # Cluster Lifecycle
  # ============================================================================
  cluster:create:
    desc: Create Kind cluster with Solo initialization
    cmds:
      - task: check
      - |
        "{{.SCRIPTS_DIR}}/solo-setup-cluster.sh" \
          --cluster-name "{{.CLUSTER_NAME}}" \
          --namespace "{{.NAMESPACE}}" \
          --deployment "{{.DEPLOYMENT}}" \
          --topology "{{.TOPOLOGY}}" \
          --topologies-dir "{{.TOPOLOGIES_DIR}}"

  cluster:destroy:
    desc: Destroy Kind cluster and clean up Solo config
    cmds:
      - |
        echo "Cleaning up Solo local config..."
        solo deployment config delete -d "{{.DEPLOYMENT}}" -q 2>/dev/null || true
        solo cluster-ref config disconnect -c "{{.CLUSTER_REF}}" -q 2>/dev/null || true

        # Fallback: if Solo CLI failed, clean config directly with yq
        if yq -e '.deployments[] | select(.name == "{{.DEPLOYMENT}}")' ~/.solo/local-config.yaml >/dev/null 2>&1; then
          echo "Solo CLI cleanup incomplete, removing stale config entry..."
          yq -i 'del(.deployments[] | select(.name == "{{.DEPLOYMENT}}"))' ~/.solo/local-config.yaml 2>/dev/null || true
          yq -i 'del(.clusterRefs["{{.CLUSTER_REF}}"])' ~/.solo/local-config.yaml 2>/dev/null || true
        fi

        echo "Destroying Kind cluster '{{.CLUSTER_NAME}}'..."
        kind delete cluster -n "{{.CLUSTER_NAME}}" || true
        echo "Cluster and Solo config cleaned up."

  # ============================================================================
  # Network Deployment
  # ============================================================================
  network:deploy:
    desc: Deploy network using configured topology
    silent: true
    cmds:
      - |
        # Resolve versions (latest -> actual GA versions)
        VERSIONS=$("{{.SCRIPTS_DIR}}/resolve-versions.sh" "{{.CN_VERSION}}" "{{.MN_VERSION}}" "{{.BN_VERSION}}")

        # Parse resolved versions
        CN_RESOLVED=$(echo "$VERSIONS" | grep "^cn_version=" | cut -d= -f2)
        MN_RESOLVED=$(echo "$VERSIONS" | grep "^mn_version=" | cut -d= -f2)
        BN_RESOLVED=$(echo "$VERSIONS" | grep "^bn_version=" | cut -d= -f2)

        "{{.SCRIPTS_DIR}}/solo-deploy-network.sh" \
          --deployment "{{.DEPLOYMENT}}" \
          --namespace "{{.NAMESPACE}}" \
          --cluster-ref "{{.CLUSTER_REF}}" \
          --topology "{{.TOPOLOGY}}" \
          --topologies-dir "{{.TOPOLOGIES_DIR}}" \
          --cn-version "$CN_RESOLVED" \
          --mn-version "$MN_RESOLVED" \
          --bn-version "$BN_RESOLVED"

  # ============================================================================
  # Port Forwarding
  # ============================================================================
  port-forward:
    desc: Set up port forwards for local access
    cmds:
      - '"{{.SCRIPTS_DIR}}/solo-port-forward.sh" --namespace "{{.NAMESPACE}}"'

  port-forward:stop:
    desc: Stop all port forwards
    cmds:
      - |
        echo "Stopping port forwards..."
        pkill -f "kubectl port-forward" 2>/dev/null || true
        echo "Port forwards stopped."

  # ============================================================================
  # Verification
  # ============================================================================
  verify:
    desc: "Verify Block Node is receiving blocks (NODE=n for specific node, default: 1)"
    vars:
      NODE: '{{.NODE | default "1"}}'
    cmds:
      - |
        PORT=$((40839 + {{.NODE}}))
        echo "Verifying Block Node {{.NODE}} on localhost:${PORT}..."
        if ! command -v grpcurl >/dev/null 2>&1; then
          echo "ERROR: grpcurl not found. Install from: https://github.com/fullstorydev/grpcurl"
          exit 1
        fi

        STATUS=$(grpcurl -plaintext -max-msg-sz 104857600 localhost:${PORT} org.hiero.block.api.BlockNodeService/serverStatus 2>&1) || {
          echo "ERROR: Could not connect to Block Node. Is port-forward running?"
          echo "Run: task port-forward"
          exit 1
        }

        echo "Block Node {{.NODE}} Status:"
        echo "$STATUS" | grep -E "(firstAvailableBlock|lastAvailableBlock)" || echo "$STATUS"

  status:
    desc: Show network status for all nodes in the topology
    cmds:
      - '"{{.SCRIPTS_DIR}}/solo-network-status.sh" --namespace "{{.NAMESPACE}}" --topology "{{.TOPOLOGY}}" --topologies-dir "{{.TOPOLOGIES_DIR}}" --context "{{.CLUSTER_REF}}" --proto-path "{{.PROTO_PATH}}"'

  # ============================================================================
  # Logs
  # ============================================================================
  logs:bn:
    desc: "View Block Node logs (NODE=n for specific node, default: 1)"
    vars:
      NODE: '{{.NODE | default "1"}}'
    cmds:
      - kubectl logs -n "{{.NAMESPACE}}" -l "app.kubernetes.io/name=block-node-{{.NODE}}" --all-containers -f

  # ============================================================================
  # Load Generation
  # ============================================================================
  load:up:
    desc: "Start NLG (NLG_CONCURRENCY, NLG_ACCOUNTS, NLG_DURATION, NLG_EXTRA_ARGS)"
    env:
      DEPLOYMENT: "{{.DEPLOYMENT}}"
      NAMESPACE: "{{.NAMESPACE}}"
      NLG_TEST_CLASS: "{{.NLG_TEST_CLASS}}"
      NLG_CONCURRENCY: "{{.NLG_CONCURRENCY}}"
      NLG_ACCOUNTS: "{{.NLG_ACCOUNTS}}"
      NLG_DURATION: "{{.NLG_DURATION}}"
      NLG_EXTRA_ARGS: "{{.NLG_EXTRA_ARGS}}"
    cmds:
      - '"{{.SCRIPTS_DIR}}/solo-load-generate.sh" start'

  load:down:
    desc: Stop NLG load generation
    env:
      DEPLOYMENT: "{{.DEPLOYMENT}}"
      NAMESPACE: "{{.NAMESPACE}}"
      NLG_TEST_CLASS: "{{.NLG_TEST_CLASS}}"
    cmds:
      - '"{{.SCRIPTS_DIR}}/solo-load-generate.sh" stop'

  # ============================================================================
  # Main Tasks
  # ============================================================================
  up:
    desc: Full setup - create cluster, deploy network, start port-forwards
    cmds:
      - task: cluster:create
      - task: network:deploy
      - task: port-forward
      - |
        echo ""
        echo "=========================================="
        echo "Network is up and running!"
        echo "=========================================="
        echo ""
        echo "Topology: {{.TOPOLOGY}}"
        echo ""
        echo "Useful commands:"
        echo "  task verify     - Check Block Node status"
        echo "  task logs:bn    - View Block Node logs"
        echo "  task load:up    - Start load generation"
        echo "  task down       - Tear down everything"

  down:
    desc: Tear down everything - stop port-forwards and destroy cluster
    cmds:
      - task: port-forward:stop
      - task: cluster:destroy
      - |
        echo ""
        echo "All resources cleaned up."
